{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Mazes Using Reinforcement Learning and Neural Networks:\n",
    "*Colton Tapparo, Evan DeBakker*\n",
    "\n",
    "Solve a maze by implementing reinforcement learning using a neural network.\n",
    "\n",
    "## Introduction\n",
    "The goal of our group project was to solve mazes using reinforcement learning implemented with neural networks. This was done by implementing the Qtable as a Qnet instead. We chose this project because we find AI in video games to be fascinating and wanted to dive deeper into neural networks specifically to gain more insight into their operation and manipulation. Using this method, we found that after sufficient training the Qnet is very effective at solving the maze it's trained for, characteristic of neural networks it takes a while to train. In most cases the end goal was worth the extra training time, yet other methods may be more efficient. Additionally, we have a method that runs a trained network but continues to train if a goal is not found in a reasonable time in order to improve and existing network for a maze. Neural networks are unpredictable due to the random nature of the moves the epsilon greedy algorithm finds, and once the network thinks a bad move is good, it can become stuck forever without a random nudge. This is especially evident in the larger maze instance. Mazes might be better solved with other learning algorithms such as A*.\n",
    "\n",
    "## Methods\n",
    "In trying to solve mazes with reinforcement learning using neural networks, the most valuable resources were those provided by class materials and the instructors. My group partner and I went into office hours several times while working through this project in order to understand where to start on solving mazes with neural networks. The hardest part was understanding how to use the network, and what each piece of the network did in order to build the table of values. One TA suggested that we take the reinforcement learning using neural networks approach to solving mazes and pointed us to the lecture 21 jupyter notebook; a solution for the towers of Hanoi using the same method we ultimately implemented.\n",
    "\n",
    "Making the switch from Towers of Hanoi to Maze running proved to be raise some challenges, as maze running is a larger and more complex task. In towers of Hanoi, the size of the game state was a 3x3 list representing 3 pegs for disks to move between. This implies that there are at most, 2 moves to make at every turn and the size of the game will always be 3x3. However, a maze will have at most, 4 moves it could possibly make and the size of the game will be nxm (any sized rectangle). In most cases, the maze will be much greater than 3x3 so we knew that this maze object had to be efficient. After all, hundreds if not thousands of these maze objects are constructed and used during Reinforcement Learning via Neural Network. Therefor we decided to make the Maze object constructor and its methods as close to constant time as possible. Notice that each of the methods in the Maze object run in constant time with the exception of startPosition and goalPosition, which search for the coordinates of 'S' and 'G' respectively. Also, notice that in trainQnet we only need to find the startPosition and goalPosition of the original maze. Therefor, we pass the startPosition and goalPosition to the constructors of every maze constructed after that, such that the rest of the mazes are constructed in constant time. In summary, if there are n number of elements in the maze we make one call to two different O(n) methods, startPosition and goalPosition. From here on out the cost of constructing mazes, finding valid moves, and making moves is constant. So while the maze object and all of it's function can be summed up as O(n), the amortized cost of using it in our algorithm is closer to constant time.\n",
    "\n",
    "Once the neural network was in order the heavy lifting had been finished. After a network has been trained it can be tested using a testing function that selects the best move from a list of moves related to the value in the Qnet. If the testing method takes more steps to solve than the dimensions of the maze, we know that it may never solve the maze unless the Qnet is changed with poor moves being negatively reinforced. Instead of reinforcing individual move values while stepping through the maze, the Qnet is passed back to trainQnet for an additional round of training until the test function solves the maze in an acceptable time. This often results is much more training than the initial arguments specify in order to first create a Qnet. Each argument in the testing function passes a constant number of repetitions, iterations, and replays; this allows for microbursts of training until the maze is solved in a number of moves that does not exceed the maximum number of elements in the maze.\n",
    "\n",
    "Storing the Qnets in text files for later use was an idea that we had delved into, yet proved to be somewhat more complicated than anticipated. This functionality is not fully implemented, yet the methods remain in the project to show how it was intended to work. The idea was that at training time the trainQnet function would search a directory of old maze data to see if it had encountered that specific maze before. The mechanism of this is fairly straight forward: using a unique hash of each maze generated by looking at consecutive elements of the same kind - i.e. walls, open spaces, start space, and goal space - then listing the number of times it appeared, a file name is generated that is used to save elements of the Qnet. To read Qnet data back into memory the same maze hash is performed and used to search the historical data directory for a file of that name. If the file is found, that data is used to create a new Qnet with the historical data plugged into each Qnet element of the new instance, then would use that for all operations instead of training new networks each time. Casting elements into the correct data type from the file then into the neural network proved to be too much difficulty to implement properly.\n",
    "\n",
    "Evan and Colton split the work so that Evan would jump right into Neural Networks and Colton would start by making the Maze object, and then provide support for Evan. In reality, we spent a lot of time working on each of these pieces in collaboration. We had some very thoughtful debate about how the maze object should work, and we spent a good portion of time seeing the TA and working separate attempts at implementing trainQnet to cover more ground and collaborate. After days of attempt after attempt, emailing back and forth bits of progress and several rewrites of the Maze object, eventually it was Evan who broke through with the working trainQnet. He implemented testQnet shortly after, and suggested that it'd be useful to hash these mazes and use their hash as a filename to store the Qnets for later use. Colton managed to finish this hashing algorithm, however the file retrieval portion of this was not finished before the deadline. We decided that we might not have time to finish the entire hashing/Qnet storage feature, however we also decided to include this portion of the code despite it not being fully developed. The idea of historical neural network storage and more efficient reinforcement during testing are the two ideas we would have liked to develop further.\n",
    "\n",
    "## Results\n",
    "In the final product we have a reliable method of solving mazes, yet it can take a lot of training to end up with a product that can reliably solve a maze. In the case of the additional training I'm not sure if it has the intended effect due to the way the trainQnet is structured. Despite the pitfalls of that methodology, it provides a consistent way to test an already trained network, and prevents any problems with hanging that might arise. This means vastly lower error rates due to the high amount of training, yet a degree of error can still be witnessed in the final output from testing the Qnet. Another problematic finding was that subsequent calls to testQnet after a successful test continue to train the network even when the last test exhibited an optimal solution path. This can be attributed to the random nature in which the moves are made, if all moves in a list are the max, then which one is returned? On larger mazes many of these rules seem to fly out the window. The harder the maze is to solve, and the more steps it takes, the less likely our training algorithm is to come up with a result in short amount of time. The larger test maze sometimes would hang for hours before solving.\n",
    "\n",
    "Before we were using testQnet, we experimented with trainQnet to understand how the neural network behaves while it is training. We experimented with repetitions, hidden layers, replays, and iterations. What we found is that the error given for each experiment is random, however some combinations of parameters seem to be correlated with better errors (lower value). Here are the results for what we experimented with. Note that for each of these experiments: epsilon = 0.5, epsilonDecayFactor = 0.99, and we are using test2 for our maze.\n",
    "\n",
    "    repetitions     replays     iterations     hidden layers     error\n",
    "        100            0           100          [10, 10, 10]     0.1516\n",
    "        100            10          100          [10, 10, 10]     0.000756\n",
    "        100            100         100          [10, 10, 10]     0.4762\n",
    "        10             10          10           [10, 10, 10]     0.3240\n",
    "        10             0           500         [2, 2, 2, 2, 2]   0.0250\n",
    "        10             10          500         [2, 2, 2, 2, 2]   1.2030\n",
    "        10             10          10             [15, 15]       0.8660\n",
    "        10             10          10             [15, 15]       1.9977\n",
    "        10             10          10             [15, 15]       0.7870\n",
    "        10             10          10            [5, 10, 15]     0.4180\n",
    "        10             10          10            [15, 10, 5]     0.1970\n",
    "        \n",
    "Observe the various error values we received. The lower these values are, the easier it was for the the neural network to find the solution. Essentially, the error values we are given are random. Our AI might run straight to the solution, producing a very small and efficient error-value. Or the AI may be incredibly unlucky, picking the wrong choice every time and causing the code to run indefinitely. If this ever happens to you, simply try running the code again. It will be unlikely for it to have that bad of luck twice in a row.\n",
    "\n",
    "The remarkable thing about this program is the error values before testQnet compared to the error values given after testQnet. When I ran the \"Testing\" section above, I was given an error value of 3.76822190084106e-15. This is incredibly more efficient than before testing. We can tell from the minuscule error value, that our AI went nearly directly to the goal. Ideally, it would be useful to store this Qnet in a file to use it to solve similar mazes. This was our initial thinking when we made the hashMaze function. The idea was to save a Qnet in a file which was named after the hashed maze value of the maze which it was trained on. This way, we can use the files name to identify the maze which it can be used to solve. Essentially, we could search for and reload an existing Qnet which was trained to run the maze rather than training an entire other Qnet. We'd locate this this Qnet using a string matching algorithm (of which there are tons of interesting optimized string matching algorithms). Unfortunately, we only thought to write that hash function at the eleventh hour, so we won't be completing that. I encourage you to check out the hashing algorithm, and ponder all of the fun ways we can use it! The output is actually very similar to some image compression algorithms.\n",
    "\n",
    "## Conclusion\n",
    "While a very interesting idea, neural networks are still impractical for many situations. These networks are computationally expensive to improve the end result, and doesn't always work. In some cases, our AI became stuck in corners until it seemed as though the code was hung, yet eventually it would find the goal. In the future I would be very interested in implementing methods of reinforcing the neural network once it has already been trained while using it with testing functions. One feature that wasn't implemented in time for the final submission was the ability to save learning data to files associated with the maze object. This was finished other than the challenge of reading all parts of memory back into a neural network from a file, which, proved to be labor and data casting intensive. The current implementation of training the Qnet further if the goal is not found in the test function may be doing redundant work and may even be making bad moves appear as though they are valid once again. In the future it would be interesting to implement a reinforcement learning neural network AI enemy in a simple side scrolling game based off this code. An enemy that learns as you fight it, and one day you can't beat it anymore, unless you change your tactic, then it has to relearn all over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "Make a list with at least four entries with dates and describe what each team member will accomplish by these dates.  This is for your use.  Your grade will not depend on meeting these deadlines.\n",
    "\n",
    "RESPONSE\n",
    "\n",
    "* Create test set of mazes (Nov. 3rd) Colton\n",
    "* write printState (Nov. 3rd) Colton\n",
    "* write getMoves (Nov. 4th) Colton\n",
    "* write neuralNet (Nov.6th) Evan\n",
    "* write learning algorithm (Nov. 6th) Evan\n",
    "* Testing and optimization (Nov. 8th) Colton and Evan\n",
    "\n",
    "I will write everything having to do with the maze, while Evan writes everything having to do with the neural network. We anticipate that I will finish the maze stuff much sooner and so I will help Evan complete his tasks once I am finished with mine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project:\n",
    "\n",
    "Solve a maze by implementing reinforcement learning using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Mazes\n",
    "\n",
    "Mazes solved by this program must...\n",
    "- Be a square nxn list\n",
    "- Start (S) at (0,0)\n",
    "- End (G) at (n,n)\n",
    "- Use '0' as path\n",
    "- Use 'W' as wall\n",
    "\n",
    "We are first getting this to work for 8x8 mazes, though we hope to generalize our code to solve any nxn maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test0 = [['S', 'W', 'W', 'W', 'W', 'W', '0', '0', '0'],\n",
    "         ['0', '0', 'W', '0', '0', 'W', '0', 'W', '0'],\n",
    "         ['0', 'W', 'W', '0', 'W', '0', '0', 'W', '0'],\n",
    "         ['0', '0', '0', '0', 'W', 'W', '0', 'W', 'G'],\n",
    "         ['W', 'W', '0', 'W', '0', '0', '0', 'W', '0'],\n",
    "         ['0', 'W', '0', '0', '0', 'W', '0', 'W', '0'],\n",
    "         ['0', 'W', '0', 'W', '0', 'W', '0', 'W', '0'],\n",
    "         ['0', '0', '0', 'W', '0', 'W', '0', '0', '0'],\n",
    "         ['0', 'W', 'W', 'W', 'W', 'W', 'W', '0', '0'],\n",
    "         ['0', '0', 'W', '0', '0', 'W', '0', '0', '0'],\n",
    "         ['0', 'W', 'W', '0', 'W', '0', '0', 'W', '0'],\n",
    "         ['0', '0', '0', '0', 'W', 'W', '0', 'W', '0'],\n",
    "         ['W', 'W', '0', 'W', '0', '0', '0', 'W', '0'],\n",
    "         ['0', 'W', '0', '0', '0', 'W', '0', 'W', '0'],\n",
    "         ['0', 'W', '0', 'W', '0', 'W', '0', 'W', '0'],\n",
    "         ['0', '0', '0', 'W', '0', 'W', '0', '0', '0']]\n",
    "\n",
    "\n",
    "test1 = [['S', '0', '0', '0', '0', '0', '0', '0'],\n",
    "         ['0', 'W', 'W', '0', 'W', 'W', 'W', '0'],\n",
    "         ['0', '0', '0', '0', '0', 'W', '0', '0'],\n",
    "         ['0', 'W', '0', '0', '0', 'W', 'W', '0'],\n",
    "         ['0', 'W', 'W', 'W', '0', 'W', '0', '0'],\n",
    "         ['0', '0', 'W', 'W', '0', '0', '0', 'W'],\n",
    "         ['0', '0', 'W', '0', '0', 'W', '0', 'W'],\n",
    "         ['0', '0', 'W', '0', '0', 'W', '0', 'G']]\n",
    "\n",
    "test2 = [['S', 'W', 'W', 'W', 'W', 'W', 'W', 'W'],\n",
    "         ['0', '0', 'W', '0', '0', 'W', '0', 'W'],\n",
    "         ['0', 'W', 'W', '0', 'W', '0', '0', 'W'],\n",
    "         ['0', '0', '0', '0', 'W', 'W', '0', 'W'],\n",
    "         ['W', 'W', '0', 'W', '0', '0', '0', 'W'],\n",
    "         ['0', 'W', '0', '0', '0', 'W', '0', 'W'],\n",
    "         ['0', 'W', '0', 'W', '0', 'W', '0', 'W'],\n",
    "         ['0', '0', '0', 'W', '0', 'W', '0', 'G']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import neuralnetworks as nn\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze Object\n",
    "\n",
    "#### Constructor Parameters:\n",
    "- maze: an nxm list of lists. You may refer to test0 and test1 as examples.\n",
    "- S_positioin: This is a list representing the coordinates of the starting position. If 'S' is at position maze[ i ] [ j ], S_position will be [ i , j ]. If S_position is not provided, the starting position will be assumed to be [ 0 , 0 ]\n",
    "- G_position: This is a list representing the coordinates of the goal position (same format as S_position). However, G_position will be assumed to be   [len(maze), len(maze[0])] if none is provided.\n",
    "\n",
    "#### Summary:\n",
    "Notice that every def in this maze object is constant time with the exception of the two static methods. Given n number of elements in the maze, the two static methods are O(n). The two static methods scan through the entire maze searching for the starting position, S, or the goal position, G. We call these only once at the beginning of trainQnet. After this, we pass S_position and G_position for every maze we create. So while the total cost of initializing a maze without knowing the starting and goal positions is O(n), the amortized cost of constructing and using a maze is constant time.\n",
    "    \n",
    "#### def Summaries:    \n",
    "- __init__ : constructor for the maze object. If you know the start position and goal position, the object is constructed in constant time. Otherwise, you'll need to utilize the static methods and set position and goalposition manually.\n",
    "- __str__ : This def formats the maze to print in a nicely spaced rectangle when you print a maze. It also prints the position and goal position of the maze.\n",
    "- __printState__ : this prints the state of the maze as it is. This is used in __str__.\n",
    "- __validMoves__ : this returns a list of coordinates which are represented as lists. These coordinates contain all of the valid places that you can move to from your current position. It's important to note that although this function looks long and complicated, it is nothing more than a series of if statements. Therefor, it is constant time.\n",
    "- __makeMove__ : this changes the element at maze[ curLoc[0], curLoc[1] ] into maze[ self.position[0], self.position[1] ]. It is important to note that __makeMove__ is constant time as well.\n",
    "- __reset__ : this returns the maze as it was in its original state. Note that it takes constant time.\n",
    "- __startPosition__ : This method iterates through every element in the maze, searching for an 'S'. Once it has found the 'S' it returns [ i, j ] given that 'S' was found at maze.state[ i ][ j ]. Note that this method is static and that it takes O(n) time given that there are n number of elements in the maze.\n",
    "- __goalPosition__ : This method iterates through every element in the maze, searching for a 'G'. Once it has found the 'G' it returns [ i, j] given that 'G' was found at maze.state[ i ][ j ]. Note that this method is static and that it takes O(n) time given that there are n number of elements in the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Maze:\n",
    "# position = (i,j) : coordinates in the maze[i][j] containing S\n",
    "#- 0 <= i,j in Sposition <=7\n",
    "#- This parameter saves the cost of locating S\n",
    "    def __init__(self, maze, S_position = None, G_position = None):\n",
    "        self.state = copy.deepcopy(maze)\n",
    "        self.rows = len(maze)\n",
    "        self.cols = len(maze[0])\n",
    "        if S_position == None:\n",
    "            self.position = [0,0]\n",
    "        else:\n",
    "            self.position = copy.copy(S_position)\n",
    "        if G_position == None:\n",
    "            self.goalposition = [len(maze),len(maze[0])]\n",
    "        else:\n",
    "            self.goalposition = copy.copy(G_position)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"State is:\\n{}\\nPosition is: {}, Goal is: {}\\n\".format(\n",
    "            self.printState(), self.position, self.goalposition)\n",
    "               \n",
    "\n",
    "    def printState(self):\n",
    "        line = \"\"\n",
    "        for i in range(0, self.rows):\n",
    "            for j in range(0, self.cols):\n",
    "                line += self.state[i][j] + \" \"\n",
    "            line += \"\\n\"\n",
    "        return line\n",
    "\n",
    "\n",
    "    def validMoves(self):\n",
    "        i, j = self.position[0], self.position[1]\n",
    "        up, down, left, right = True, True, True, True\n",
    "        if i == 0:\n",
    "            up = False\n",
    "        if i == self.rows-1:\n",
    "            down = False\n",
    "        if j == 0:\n",
    "            left = False\n",
    "        if j == self.cols-1:\n",
    "            right = False\n",
    "\n",
    "        result = []\n",
    "        # Check for walls\n",
    "        found = False\n",
    "        if up == True:\n",
    "            if self.state[i-1][j]  == '0' or self.state[i-1][j]  == 'G':\n",
    "                result.append([i-1,j])\n",
    "                found = True\n",
    "        if down == True:\n",
    "            if self.state[i+1][j] == '0' or self.state[i+1][j]  == 'G':\n",
    "                result.append([i+1,j])\n",
    "                found = True\n",
    "        if left == True:\n",
    "            if self.state[i][j-1] == '0' or self.state[i][j-1]  == 'G':\n",
    "                result.append([i,j-1])\n",
    "                found = True\n",
    "        if right == True:\n",
    "            if self.state[i][j+1] == '0' or self.state[i][j+1]  == 'G':\n",
    "                result.append([i,j+1])\n",
    "                found = True\n",
    "                \n",
    "        if not found:\n",
    "            if up == True:\n",
    "                if self.state[i-1][j]  == 'B':\n",
    "                    result.append([i-1,j])\n",
    "            if down == True:\n",
    "                if self.state[i+1][j] == 'B':\n",
    "                    result.append([i+1,j])\n",
    "            if left == True:\n",
    "                if self.state[i][j-1] == 'B':\n",
    "                    result.append([i,j-1])\n",
    "            if right == True:\n",
    "                if self.state[i][j+1] == 'B':\n",
    "                    result.append([i,j+1])\n",
    "            \n",
    "        return result\n",
    "\n",
    "    \n",
    "    # Makes the specified move on this maze\n",
    "    def makeMove(self, curLoc): \n",
    "        iNew = self.position[0]\n",
    "        jNew = self.position[1]\n",
    "        iOld = curLoc[0]\n",
    "        jOld = curLoc[1]\n",
    " \n",
    "        self.state[iOld][jOld] = 'B'\n",
    "        self.state[iNew][jNew] = 'S'\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self, state, startposition, goalposition):\n",
    "        del(self)\n",
    "        return Maze(state, startposition, goalposition)\n",
    "    \n",
    "    @staticmethod\n",
    "    def startPosition(state):\n",
    "        for i in range(len(state)):\n",
    "            for j in range(len(state[0])):  \n",
    "                if state[i][j] == 'S':\n",
    "                    return [i,j]\n",
    "    \n",
    "    @staticmethod\n",
    "    def goalPosition(state):\n",
    "        for i in range(len(state)):\n",
    "            for j in range(len(state[0])):  \n",
    "                if state[i][j] == 'G':\n",
    "                    return[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State is:\n",
      "S W W W W W 0 0 0 \n",
      "0 0 W 0 0 W 0 W 0 \n",
      "0 W W 0 W 0 0 W 0 \n",
      "0 0 0 0 W W 0 W G \n",
      "W W 0 W 0 0 0 W 0 \n",
      "0 W 0 0 0 W 0 W 0 \n",
      "0 W 0 W 0 W 0 W 0 \n",
      "0 0 0 W 0 W 0 0 0 \n",
      "0 W W W W W W 0 0 \n",
      "0 0 W 0 0 W 0 0 0 \n",
      "0 W W 0 W 0 0 W 0 \n",
      "0 0 0 0 W W 0 W 0 \n",
      "W W 0 W 0 0 0 W 0 \n",
      "0 W 0 0 0 W 0 W 0 \n",
      "0 W 0 W 0 W 0 W 0 \n",
      "0 0 0 W 0 W 0 0 0 \n",
      "\n",
      "Position is: [0, 0], Goal is: [3, 8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(test0, Maze.startPosition(test0), Maze.goalPosition(test0))\n",
    "print(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epsilonGreedy\n",
    "\n",
    "This epsilonGreedy def was provided by Chuck Anderson in jupyter notebook 21, the one about implementing Reinforcement learning using a Neural Network. It has been modified to work with our maze objects. Rather than passing around states and functions, we modified it to pass the maze and use the maze methods.\n",
    "\n",
    "Arguments:\n",
    "- Q\n",
    "- maze : a maze object\n",
    "- epsilon: a floating point number between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilonGreedy(Qnet, maze, epsilon):\n",
    "    moves = maze.validMoves()\n",
    "    if np.random.uniform() < epsilon: # random move\n",
    "        move = moves[random.sample(range(len(moves)),1)[0]]\n",
    "        Q = Qnet.use(np.array([maze.position + move])) if Qnet.Xmeans is not None else 0\n",
    "    else:                           # greedy move\n",
    "        qs = []\n",
    "        for m in moves:\n",
    "            qs.append(Qnet.use(np.array([maze.position + m])) if Qnet.Xmeans is not None else 0)\n",
    "        move = moves[np.argmax(qs)]\n",
    "        Q = np.max(qs)\n",
    "    return move, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainQnet\n",
    "\n",
    "This trainQnet def was also provided by Chuck Anderson in jupyter notebook 21. However, we have modified it to use maze objects and to rely on the maze methods rather than using functions passed in as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainQnet(nReps, maze, hiddenLayers, nIterations, nReplays, epsilon, epsilonDecayFactor, Qnet = None):\n",
    "    outcomes = np.zeros(nReps)\n",
    "    if Qnet == None:\n",
    "        Qnet = nn.NeuralNetwork(4, hiddenLayers, 1)\n",
    "        Qnet._standardizeT = lambda x: x\n",
    "        Qnet._unstandardizeT = lambda x: x\n",
    "        start = True\n",
    "    else:\n",
    "        start = False\n",
    "    goalposition = Maze.goalPosition(maze.state)\n",
    "    startState = copy.deepcopy(maze.state)\n",
    "    startposition = Maze.startPosition(maze.state)\n",
    "    # epsilon = 1.0\n",
    " \n",
    "    samples = []  # collect all samples for this repetition, then update the Q network at end of repetition.\n",
    "    for rep in range(nReps):\n",
    "        if rep > 0:\n",
    "            epsilon *= epsilonDecayFactor\n",
    "        step = 0\n",
    "        done = False\n",
    " \n",
    "        samples = []\n",
    "        samplesNextStateForReplay = []\n",
    "       \n",
    "    # 1.) start state needs to be changed for mazes (DONE)#\n",
    "        state = maze.state\n",
    "        move, _ = epsilonGreedy(Qnet, maze, epsilon)\n",
    " \n",
    "        while not done:\n",
    "            step += 1\n",
    "           \n",
    "           # Make this move to get to nextState\n",
    "            mazeNext = Maze(copy.deepcopy(maze.state), move, goalposition)\n",
    "            mazeNext.makeMove(maze.position)\n",
    "            \n",
    "            r = -1\n",
    "            # Choose move from mazeNext\n",
    "            moveNext, Qnext = epsilonGreedy(Qnet, mazeNext, epsilon)\n",
    "        # 2.) this will need to be changed for the mazes #\n",
    "            if goalposition == mazeNext.position: # Check if goal is reached\n",
    "                if rep == nReps-1:\n",
    "                    if start:\n",
    "                        print('goal found')\n",
    "                        print(mazeNext)\n",
    "                # goal found\n",
    "                Qnext = 0\n",
    "                done = True \n",
    "                outcomes[rep] = step\n",
    "                mazeNext = mazeNext.reset(startState, startposition, goalposition)\n",
    "            samples.append([*maze.position, *move, r, Qnext])\n",
    "            samplesNextStateForReplay.append([*mazeNext.position, *moveNext])\n",
    " \n",
    "            maze = copy.deepcopy(mazeNext)\n",
    "            move = copy.copy(moveNext)\n",
    "           \n",
    "        samples = np.array(samples)\n",
    "    # 3.) this will need to be changed for the mazes #\n",
    "        X = samples[:,:4]\n",
    "        T = samples[:,4:5] + samples[:,5:6]\n",
    "        Qnet.train(X, T, nIterations, verbose=False)\n",
    " \n",
    "        # Experience Replay: Train on recent samples with updates to Qnext.\n",
    "        samplesNextStateForReplay = np.array(samplesNextStateForReplay)\n",
    "        for replay in range(nReplays):\n",
    "            QnextNotZero = samples[:,5] != 0\n",
    "            samples[QnextNotZero, 5:6] = Qnet.use(samplesNextStateForReplay[QnextNotZero,:])\n",
    "            T = samples[:,4:5] + samples[:,5:6]\n",
    "            Qnet.train(X, T, nIterations, verbose=False)\n",
    " \n",
    "    return Qnet, outcomes, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Qnet\n",
    "A method for testing trained Qnets. If it doesn't solve it, train it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findMove(maze, Qnet):\n",
    "    moves = maze.validMoves()\n",
    "    qs = []\n",
    "    for m in moves:\n",
    "        qs.append(Qnet.use(np.array([maze.position + m])) if Qnet.Xmeans is not None else 0)\n",
    "        move = moves[np.argmax(qs)]\n",
    "        Q = np.max(qs)\n",
    "    return move, Q\n",
    "    \n",
    "\n",
    "def testQnet(maze, Qnet, hiddenLayers):\n",
    "    goalposition = Maze.goalPosition(maze.state)\n",
    "    startState = copy.deepcopy(maze.state)\n",
    "    startposition = Maze.startPosition(maze.state)\n",
    "    done, start = False, True\n",
    "    move, _ = findMove(maze, Qnet)\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        if steps > maze.cols*maze.rows:\n",
    "            if start:\n",
    "                sys.stdout.write('Goal not found, training')\n",
    "                start = False\n",
    "            else:\n",
    "                sys.stdout.write('.')\n",
    "            Qnet, _, __ = trainQnet(30, maze, hiddenLayers, 10, 10, 0.5, 0.99, Qnet)\n",
    "        # Make this move to get to nextState\n",
    "        mazeNext = Maze(copy.deepcopy(maze.state), move, goalposition)\n",
    "        mazeNext.makeMove(maze.position)\n",
    "        # Choose move from mazeNext\n",
    "        move, _ = findMove(maze, Qnet)\n",
    "        moveNext, Qnext = findMove(mazeNext, Qnet)\n",
    "        if goalposition == mazeNext.position: # Check if goal is reached\n",
    "            print()\n",
    "            print('goal found')\n",
    "            print(mazeNext)\n",
    "            # goal found\n",
    "            Qnext = 0\n",
    "            done = True \n",
    "            mazeNext = mazeNext.reset(startState, startposition, goalposition)\n",
    "        maze = copy.deepcopy(mazeNext)\n",
    "        move = copy.copy(moveNext)\n",
    "\n",
    "        \n",
    "def printResults(results):\n",
    "    print(\"{}\\n{}\\n{}\".format(results[0], results[1], results[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hashMaze\n",
    "\n",
    "#### Parameter:\n",
    "- __state__ : This is the 2D list which represents the maze, not the entire maze object.\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "hashMaze is a hashing function which condenses the layout of the maze into one string. It searches through the maze line by line, counting the number of consecutive coordinates which hold an equal value and appends the value and the number onto a string. For Example, if I had maze which looked like...\n",
    "    \n",
    "    - [ S, W, W ]\n",
    "    - [ 0, 0, W ]\n",
    "    - [ W, 0, G ]\n",
    "    \n",
    "    ... then the resulting hash would look like...\n",
    "    \n",
    "    S1W202W1W101G1\n",
    "    \n",
    "    \n",
    "This hash will be used as a file name to store the training data of the neural net which trained on this maze. We can use this training data later to solve other mazes, allowing it to learn more.\n",
    "\n",
    "* Not all of the functionality of this section works right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def hashMaze(maze):\n",
    "    buf = ''\n",
    "    hsh = ''\n",
    "    for i in range(maze.rows):\n",
    "        for j in range(maze.cols):\n",
    "            if buf == '' or buf[0] == str(maze.state[i][j]):\n",
    "                buf += str(maze.state[i][j])\n",
    "            elif len(buf) > 0:\n",
    "                hsh += str(buf[0]) + str(len(buf))\n",
    "                buf = str(maze.state[i][j])\n",
    "            if i == maze.rows-1 and j == maze.cols-1 and len(buf) > 0:\n",
    "                hsh += str(buf[0]) + str(len(buf))\n",
    "            #print('i: {}, j: {}, state[i][j]: {}, buffer is: {}'.format(i, j, maze.state[i][j], buf))\n",
    "    return hsh\n",
    "\n",
    "def storeQnet(maze, Qnet):\n",
    "    directory = 'mazeData'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file = open(directory + '/' + hashMaze(maze), 'w')\n",
    "    output = (str(Qnet.ni)+','+str(Qnet.nhs)+','+str(Qnet.no)+','+str(Qnet.Xmeans)+','+str(Qnet.Xstds)\n",
    "              +','+str(Qnet.Tmeans)+','+str(Qnet.Tstds)+','+str(Qnet.trained)+','+str(Qnet.reason)\n",
    "              +','+str(Qnet.errorTrace)+','+str(Qnet.numberOfIterations)\n",
    "              +','+str(Qnet.Vs).replace('(', ',').replace(')', ',').replace(',','')+','+str(Qnet.W)\n",
    "              +','+str(Qnet.Xconstant)+','+str(Qnet.XstdsFixed)+','+str(Qnet.Tconstant)+','+str(Qnet.TstdsFixed))\n",
    "    output = output.replace('[', '').replace(']', '').replace('array', '')\n",
    "    file.write(output)\n",
    "    file.close()\n",
    "    \n",
    "def charToFloat(ls):\n",
    "    return [float(x) for x in ls]\n",
    "\n",
    "def charToInt(ls):\n",
    "    return [int(x) for x in ls]\n",
    "    \n",
    "def readQnet(maze):\n",
    "    directory = 'mazeData'\n",
    "    path = directory + '/' + hashMaze(maze)\n",
    "    if not os.path.exists(directory):\n",
    "        return None\n",
    "    elif not os.path.isfile(path):\n",
    "        return None\n",
    "    else:\n",
    "        file = open(path, 'r')\n",
    "        output = file.read()\n",
    "        file.close()\n",
    "        output = output.split(',')\n",
    "        output = [x.split() for x in output]\n",
    "        ni,nhs,no,trained,reason = int(output[0][0]),charToInt(output[1]),int(output[2][0]),output[7],output[8]\n",
    "        Xconst, Tconst = output[13], output[15]\n",
    "        output = output[3:7] + output[9:11] + output[14] + output[16]\n",
    "        output = [charToFloat(x) for x in output]\n",
    "        Qnet = nn.NeuralNetwork(no, nhs, ni)\n",
    "        Qnet.Xmeans = output[0]\n",
    "        Qnet.Xstds = output[1]\n",
    "        Qnet.Tmeans = output[2]\n",
    "        Qnet.Tstds = output[3]\n",
    "        Qnet.trained = trained\n",
    "        Qnet.reason = reason\n",
    "        Qnet.errorTrace = output[4]\n",
    "        Qnet.numberOfIterations = output[5]\n",
    "        return Qnet\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S1W702W102W101W101W201W102W104W201W301W103W101W103W101W101W101W101W101W103W101W101G1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hMaze = Maze(test2)\n",
    "hashMaze(hMaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal found\n",
      "State is:\n",
      "B W W W W W W W \n",
      "B B W B B W B W \n",
      "B W W B W B B W \n",
      "B B B B W W B W \n",
      "W W B W B B B W \n",
      "0 W B B B W B W \n",
      "0 W 0 W 0 W B W \n",
      "0 0 0 W 0 W B S \n",
      "\n",
      "Position is: [7, 7], Goal is: [7, 7]\n",
      "\n",
      "Goal not found, training...........\n",
      "goal found\n",
      "State is:\n",
      "B W W W W W W W \n",
      "B B W 0 0 W 0 W \n",
      "B W W 0 W 0 0 W \n",
      "B B B 0 W W 0 W \n",
      "W W B W B B B W \n",
      "0 W B B B W B W \n",
      "0 W 0 W B W B W \n",
      "0 0 0 W B W B S \n",
      "\n",
      "Position is: [7, 7], Goal is: [7, 7]\n",
      "\n",
      "Goal not found, training.....................\n",
      "goal found\n",
      "State is:\n",
      "B W W W W W W W \n",
      "B B W 0 0 W B W \n",
      "B W W 0 W B B W \n",
      "B B B 0 W W B W \n",
      "W W B W B B B W \n",
      "B W B B B W B W \n",
      "B W B W 0 W B W \n",
      "B B B W 0 W B S \n",
      "\n",
      "Position is: [7, 7], Goal is: [7, 7]\n",
      "\n",
      "Goal not found, training...................\n",
      "goal found\n",
      "State is:\n",
      "B W W W W W W W \n",
      "B B W B B W 0 W \n",
      "B W W B W 0 0 W \n",
      "B B B B W W 0 W \n",
      "W W B W B B B W \n",
      "0 W B B B W B W \n",
      "0 W 0 W 0 W B W \n",
      "0 0 0 W 0 W B S \n",
      "\n",
      "Position is: [7, 7], Goal is: [7, 7]\n",
      "\n",
      "NeuralNetwork(4, [40], 1)\n",
      "   Network was trained for 1 iterations. Final error is 2.7557006760850673e-11.\n",
      "[ 210.   30.   50.   20.  132.   36.   50.   18.   38.   20.   42.   18.\n",
      "   32.   26.   24.   54.   34.   24.  170.   92.   28.   42.  192.   28.\n",
      "  424.   44.   18.   16.   58.  138.]\n",
      "[[  0.           0.           1.           0.          -1.         -24.95802587]\n",
      " [  1.           0.           1.           1.          -1.         -23.63792607]\n",
      " [  1.           1.           1.           0.          -1.         -23.60320669]\n",
      " [  1.           0.           2.           0.          -1.         -22.88331201]\n",
      " [  2.           0.           3.           0.          -1.         -25.89529167]\n",
      " [  3.           0.           3.           1.          -1.         -25.78182955]\n",
      " [  3.           1.           3.           2.          -1.         -26.63838445]\n",
      " [  3.           2.           3.           3.          -1.         -28.11696478]\n",
      " [  3.           3.           2.           3.          -1.         -28.01856486]\n",
      " [  2.           3.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -26.81687897]\n",
      " [  1.           3.           2.           3.          -1.         -28.01856486]\n",
      " [  2.           3.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -26.81687897]\n",
      " [  1.           3.           2.           3.          -1.         -24.42310993]\n",
      " [  2.           3.           3.           3.          -1.         -28.11696478]\n",
      " [  3.           3.           2.           3.          -1.         -28.01856486]\n",
      " [  2.           3.           1.           3.          -1.         -26.81687897]\n",
      " [  1.           3.           2.           3.          -1.         -28.01856486]\n",
      " [  2.           3.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -26.81687897]\n",
      " [  1.           3.           2.           3.          -1.         -28.01856486]\n",
      " [  2.           3.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -26.81687897]\n",
      " [  1.           3.           2.           3.          -1.         -28.01856486]\n",
      " [  2.           3.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -26.81687897]\n",
      " [  1.           3.           2.           3.          -1.         -28.01856486]\n",
      " [  2.           3.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -26.81687897]\n",
      " [  1.           3.           2.           3.          -1.         -28.01856486]\n",
      " [  2.           3.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -26.81687897]\n",
      " [  1.           3.           2.           3.          -1.         -28.01856486]\n",
      " [  2.           3.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -27.84778669]\n",
      " [  1.           3.           1.           4.          -1.         -27.58193663]\n",
      " [  1.           4.           1.           3.          -1.         -26.81687897]\n",
      " [  1.           3.           2.           3.          -1.         -24.42310993]\n",
      " [  2.           3.           3.           3.          -1.         -22.0624405 ]\n",
      " [  3.           3.           3.           2.          -1.         -19.53789645]\n",
      " [  3.           2.           4.           2.          -1.         -17.80448786]\n",
      " [  4.           2.           5.           2.          -1.         -18.85267515]\n",
      " [  5.           2.           5.           3.          -1.         -21.5250294 ]\n",
      " [  5.           3.           5.           4.          -1.         -19.83084784]\n",
      " [  5.           4.           4.           4.          -1.         -19.59463438]\n",
      " [  4.           4.           4.           5.          -1.         -19.89549499]\n",
      " [  4.           5.           4.           6.          -1.         -18.77088939]\n",
      " [  4.           6.           3.           6.          -1.         -17.00621063]\n",
      " [  3.           6.           2.           6.          -1.         -19.72109513]\n",
      " [  2.           6.           2.           5.          -1.         -17.01715442]\n",
      " [  2.           5.           2.           6.          -1.         -20.01751573]\n",
      " [  2.           6.           1.           6.          -1.         -19.48972784]\n",
      " [  1.           6.           2.           6.          -1.         -20.01751573]\n",
      " [  2.           6.           1.           6.          -1.         -19.48972784]\n",
      " [  1.           6.           2.           6.          -1.         -19.72109513]\n",
      " [  2.           6.           2.           5.          -1.         -17.01715442]\n",
      " [  2.           5.           2.           6.          -1.          -9.47931103]\n",
      " [  2.           6.           3.           6.          -1.          -6.70758213]\n",
      " [  3.           6.           4.           6.          -1.          -6.75663899]\n",
      " [  4.           6.           5.           6.          -1.          -4.37976444]\n",
      " [  5.           6.           6.           6.          -1.          -2.84271687]\n",
      " [  6.           6.           7.           6.          -1.          -2.01343453]\n",
      " [  7.           6.           7.           7.          -1.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(test2, Maze.startPosition(test2), Maze.goalPosition(test2))\n",
    "hiddenLayers = [40]\n",
    "results = trainQnet(30, maze, hiddenLayers, 10, 10, 0.5, 0.99)\n",
    "testQnet(maze, results[0], hiddenLayers)\n",
    "testQnet(maze, results[0], hiddenLayers)\n",
    "testQnet(maze, results[0], hiddenLayers)\n",
    "printResults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal found\n",
      "State is:\n",
      "B B B B B B B B \n",
      "0 W W 0 W W W B \n",
      "0 0 0 0 0 W 0 B \n",
      "0 W 0 0 0 W W B \n",
      "0 W W W 0 W B B \n",
      "0 0 W W 0 0 B W \n",
      "0 0 W 0 0 W B W \n",
      "0 0 W 0 0 W B S \n",
      "\n",
      "Position is: [7, 7], Goal is: [7, 7]\n",
      "\n",
      "\n",
      "goal found\n",
      "State is:\n",
      "B B B B 0 0 0 0 \n",
      "0 W W B W W W 0 \n",
      "0 0 B B 0 W 0 0 \n",
      "0 W B B B W W 0 \n",
      "0 W W W B W 0 0 \n",
      "0 0 W W B B B W \n",
      "0 0 W 0 0 W B W \n",
      "0 0 W 0 0 W B S \n",
      "\n",
      "Position is: [7, 7], Goal is: [7, 7]\n",
      "\n",
      "NeuralNetwork(4, [5, 5, 5], 1)\n",
      "   Network was trained for 21 iterations. Final error is 0.4805815277143402.\n",
      "[  148.    38.    84.    14.    54.    64.  3670.   198.    70.    50.\n",
      "   132.    18.    16.   750.    14.    36.    44.    52.    16.    16.]\n",
      "[[  0.           0.           0.           1.          -1.         -18.92029302]\n",
      " [  0.           1.           0.           2.          -1.         -18.40620513]\n",
      " [  0.           2.           0.           3.          -1.         -18.02336201]\n",
      " [  0.           3.           0.           4.          -1.         -18.80579107]\n",
      " [  0.           4.           0.           5.          -1.         -19.06364966]\n",
      " [  0.           5.           0.           6.          -1.         -17.02714867]\n",
      " [  0.           6.           0.           7.          -1.         -14.91616949]\n",
      " [  0.           7.           1.           7.          -1.         -13.16332245]\n",
      " [  1.           7.           2.           7.          -1.          -9.25122546]\n",
      " [  2.           7.           3.           7.          -1.          -5.2470298 ]\n",
      " [  3.           7.           4.           7.          -1.          -4.74181696]\n",
      " [  4.           7.           4.           6.          -1.          -4.01672712]\n",
      " [  4.           6.           5.           6.          -1.          -2.91169279]\n",
      " [  5.           6.           6.           6.          -1.          -2.43973397]\n",
      " [  6.           6.           7.           6.          -1.          -2.21237605]\n",
      " [  7.           6.           7.           7.          -1.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(test1, Maze.startPosition(test1), Maze.goalPosition(test1))\n",
    "hiddenLayers = [5, 5, 5]\n",
    "results = trainQnet(20, maze, hiddenLayers, 20, 20, 0.5, 0.99)\n",
    "testQnet(maze, results[0], hiddenLayers)\n",
    "printResults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal found\n",
      "State is:\n",
      "B W W W W W B B B \n",
      "B 0 W 0 0 W B W B \n",
      "B W W 0 W B B W B \n",
      "B B B 0 W W B W S \n",
      "W W B W B B B W 0 \n",
      "0 W B B B W B W 0 \n",
      "0 W B W B W B W 0 \n",
      "B B B W B W B B 0 \n",
      "B W W W W W W B 0 \n",
      "B B W 0 0 W B B 0 \n",
      "B W W 0 W 0 B W 0 \n",
      "B B B 0 W W B W 0 \n",
      "W W B W B B B W 0 \n",
      "B W B B B W 0 W 0 \n",
      "B W B W 0 W 0 W 0 \n",
      "B B B W 0 W 0 0 0 \n",
      "\n",
      "Position is: [3, 8], Goal is: [3, 8]\n",
      "\n",
      "S1W505W102W101W102W201W102W105W201W1G1W201W103W102W103W101W102W101W101W101W104W101W104W604W102W104W201W102W105W201W101W201W103W102W103W101W102W101W101W101W104W101W103\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(test0, Maze.startPosition(test0), Maze.goalPosition(test0))\n",
    "hiddenLayers = [10, 10]\n",
    "results = trainQnet(30, maze, hiddenLayers, 10, 10, 0.5, 0.99)   \n",
    "storeQnet(maze, results[0])\n",
    "print(hashMaze(maze))\n",
    "#Qnet = readQnet(maze)\n",
    "#if Qnet is not None:\n",
    "    #testQnet(maze, Qnet, hiddenLayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    "\n",
    "[Chuck Anderson] Jupyter Notebook, \"21 Reinforcement Learning with a Neural Network as the Q Function\", http://nbviewer.jupyter.org/url/www.cs.colostate.edu/~anderson/cs440/notebooks/21%20Reinforcement%20Learning%20with%20a%20Neural%20Network%20as%20the%20Q%20Function.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
